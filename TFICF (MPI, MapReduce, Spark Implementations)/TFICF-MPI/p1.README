Single Author Info:
srames22 Srivatsan Ramesh

The goal of this program was to compute the TFICF values using the given formulae for each word in a corpus of documents via an MPI implementation to enable sharing of data of each word to the different worker nodes computing the above-mentioned TFICF values. The root rank in the set of processors acts as the master that divides the documents in the corpus to the worker nodes who in turn communicate with each other to compute the TFICF values of each word. Below is an explanation of the implementation approach undertaken by me. (Split into 5 phases)

Phase 1: Reading the documents in the corpus
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- This program makes the master perform calculations on the number of documents that must be handled by each rank and the document names and sends it to the respective worker rank(assumption: all documents have the same name with different numbers eg.: doc1, doc2, etc.). If the number of documents are evenly divisible by the number of ranks/processors then each rank gets the same number of documents to handle and the starting number of the first document that the rank must handle. This is done using an **MPI_Bcast** operation. (rank 1: doc1, doc2 => starting doc number: 1 || rank 2: doc3, doc4 => starting doc number: 3 || etc.). 

- If the number of documents are not evenly divisible, initialy each rank is assigned equal documents and the remaining documents are computed. Each one of the extra documents are then assigned to each rank until they are fully exhausted. (Assumption: Number of worker ranks can never be greater than the number of documents leaving a rank idle). The number of documents to be handled by each rank is computed in an array (number_of_docs_per_rank_array[]) by the master node and sent using blocking **MPI_Send and MPI_Recv** in a loop to all the worker ranks by the master rank. This is sent along with the starting number of document for each rank which is also computed in the array and sent using **MPI_Send and MPI_Recv** by master to all worker ranks in a loop.

- Once the number of documents and starting index of each document is received by each rank, all the worker ranks excpet the master ranks reads its set of documents to begin calculations.

Phase 2: Computing Required Data for Each Word in Each Worker Rank
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- The logic is to loop through each document and gather TFICF variables such as numDocsWithWord, word, DocSize, etc. for each word after which each rank caluclates the TFICF array of structs and unique_words array of structs for its set of documents. Each worker node/rank does the following using the TFICF and unique_word_struct for its set of documents.

- This part of the implementation is similar to that of the serial code except each rank enforces the above, not for all the documents in the corpus but for their assigned documents alone.

- Each rank will have the required variables ready for TFICF calculation except for the numDocsWithWord as a word may be present in documents handled by other ranks and hence MPI Communication is necessary here to send the numDocsWithWord for each word.

Phase 3: Updating numDocsWithWord using MPI_Igatherv
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- The array of struct to be gathered (unique_words strcut from each rank) is set up using the MPI_Datatype, MPI_Type_create_struct which is a flexible way of creating an MPI_Datatype as a sequence of blocks and committing the created Datatype using MPI_Type_commit. 

- Before gathering the array of struct at the root, the number of words handled by each rank in the struct is sent using MPI_Isend and received by the root using MPI_Irecv in an array. This array stores the total number of words from all worker ranks in thier respective index position. This is used to compute the total number of words from all the worker ranks and is used as the "count_array" in the MPI_Igatherv. It is also used to compute the "displacement array" needed for the gather.

- Finally, a gather is done: **MPI_Igatherv(unique_words, uw_idx, custom_uw_type, unique_word_at_rank0, count_array_for_gather1, disp_array_for_gather1, custom_uw_type, 0, MPI_COMM_WORLD, &req_for_gather1);** 
Each array of struct called unique_words is gathered from each rank and is stored rank wise in the array of struct defined for a much larger size at unique_word_at_rank0.

Reference: https://www.rookiehpc.com/mpi/docs/mpi_type_create_struct.php (Reference for MPI)

Phase 4: Updating each word's numDocsWithWord at unique_word_at_rank0
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- Now that all the words and its struct is there in the array of struct unique_word_at_rank0, I loop through each word and find the first occurance of a word in the same struct as that word may be found in other documents handled by other ranks. If there exists another occurance of the same word, then the numDocsWithWord values for both these occurances are added and the second occurance is set to 0 so as to not count it again. Similarly, all other occurances of the same word is handled and finally the updated value of numDocsWithWord is obtained for all words.

- Logic: Send back the updated struct at rank 0 with all the words to every worker rank which will use the structure to calculate the TFICF values just like in the serial code.

- Prior to the struct being sent, the total number of elements/word in the unique_word_at_rank0 is passed using **MPI_Isend and MPI_Irecv** from rank 0 to all other worker ranks so as to make an array of struct of that size in all other ranks.

- Each index of struct from the array of structs at rank 0 is sent to all other ranks in a nested for loop (line 385) using MPI_Isend and received at all ranks in their respective array of struct called unique_word_at_each_rank[]. Instead of sending the entire array of struct as a whole to each rank, an MPI_Isend and MPI_Irecv is carried out for each index value of the array, i.e each struct in the array.

Reference: https://stackoverflow.com/questions/18165277/how-to-send-a-variable-of-type-struct-in-mpi-send

Phase 5: Computation of TFICF and Final Gathering for Sorting
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- Just like in the serial code after receiving the updated array of struct at unique_word_at_each_rank, normal TF job and ICF job is carried out and the results are stored in the strings array.

- The strings array is being updated for each rank and must be gathered in the end for sorting by a single rank which is the master rank. (rank 0)

- For this, the total number of words (TF_idx of each rank) in the TFICF struct of each rank is sent and stored in an array in rank 0. This is used to compute the total number of elements that are going to be present at rank 0's strings array which is going to be sorted and printed in output.txt

- Finally, an MPI_Datatype is being created/set to send the strings array using the MPI_Type_contiguous (https://www.codingame.com/playgrounds/47058/have-fun-with-mpi-in-c/derived-datatypes).

- The strings array is gathered using an **MPI_Igatherv** once again with the help of the count_array and the displacement array that is need for such a call.

- Since all the array of strings for each rank is now in rank 0, and the total number of elements that have been sent are stored in the array and have been computed in the "total_tf_idx" variable, that is used in qsort function mentioned in the serial code and in the loop to print string[i] in output.txt

Adding Parallelism
~~~~~~~~~~~~~~~~~~~
We can add more parallelism to each node that uses MPI by involving concepts that we have come accross before namely OpenMP. We can use OpenMP directives by first adding the omp.h header file and find areas of the code that can be parallelized by using the OpenMP directives. We can specify the loops that we want to parallelize using the OpenMP parallel loop directive. The first loop where this can be applied is the one where rank 0 looping through the ranks and assigns equal number of documents to each rank (for(int i = 0; i < numProcs - 1; i++ ), line 116). This for loop that handles the extra documents case can also be parallelized in rank 0 (for (int i = 0; i < total_extra_docs; i++), line 122).

In a similar fashion all the loops that are to be run in each node whether it is the worker node or the master node, all the for loops can be parallelized using OpenMP constructs. 

Comparison of MPI Implementation to other Implementation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Using the MapReduce and Spark implementations of the TFICF computation of all the words in the corpus involved the use of Mapper function and the Reducer function that splits the input data set into completely independent chunks of data that are also completely run in parallel by the workers. The Mappers tend to bring the data in a key/value pair format while the reducer performs operations like summations for each key. Instead, here we utilize structures for each word so the data structure with which we store the values such as numDocsWithWord, numDocs, wordCount, etc. varies. It was using tuples in the other implementations. The mapping and reducing functions are distributed for each job, over a group of nodes in Hadoop. Frequently, at regular intervals the status of each worker node is reported back to the master node in terms of the node health. i.e whether it is still functioning or if it has been stopped and needs to be restarted or substituted with other node. In my MPI implementation the master node will assign which document must be handled by each worker and does not keep track of whether the worker nodes are functioning or not. Moreover, many parts of the code are also carried out by the master amongst other nodes. Additionaly, unlike in the MapReduce and Spark implementations the master node also computes the number of documents and communicates which node handles how much work load as well. Despite the contrasting behaviour, the way in which all implementations compute is the same however, the MPI communication could take a bit longer die to the fact that to compute the numDocsWithWords the master node gathers all the struct of arrays from each worker node, computes and updates the value based on all the occurances of that word, and sends it back to all the worker nodes. This sort of computation does not take place in the MapReduce and Spark implementations as the worker node takes care of all computations. After sending the struct of arrays with updated values using MPI, the worker nodes continue with computations of TFICF. The String is gathered again by the master using Gather which does not take place in the other two implementations. 
The final output is written out using the Hadoop File System while in the MPI implementation just using Igatherv to communicate each worker's strings array and prints them from the master node to the output.txt file.

